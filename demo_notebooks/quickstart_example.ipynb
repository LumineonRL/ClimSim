{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climsim_utils.data_utils import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_path = os.getcwd()\n",
    "root_path = os.path.dirname(notebook_path)\n",
    "\n",
    "grid_path = os.path.join(root_path, 'grid_info', 'ClimSim_low-res_grid-info.nc')\n",
    "norm_path = os.path.join(root_path, 'preprocessing', 'normalizations/')\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)\n",
    "\n",
    "# set variables to V1 subset\n",
    "data.set_to_v1_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to your own\n",
    "project_path = os.path.dirname(root_path)\n",
    "data_path = os.path.join(project_path, 'data/')\n",
    "\n",
    "train_input_path = data_path + 'train_input.npy'\n",
    "train_target_path = data_path + 'train_target.npy'\n",
    "val_input_path = data_path + 'val_input.npy'\n",
    "val_target_path = data_path + 'val_target.npy'\n",
    "\n",
    "data.input_train = data.load_npy_file(train_input_path)\n",
    "data.target_train = data.load_npy_file(train_target_path)\n",
    "data.input_val = data.load_npy_file(val_input_path)\n",
    "data.target_val = data.load_npy_file(val_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_TO_KEEP = 384*20\n",
    "\n",
    "np.save(\"train_input_small\", data.input_train[:ROWS_TO_KEEP, :])\n",
    "np.save(\"train_target_small\", data.target_train[:ROWS_TO_KEEP, :])\n",
    "np.save(\"val_input_small\", data.input_val[:ROWS_TO_KEEP, :])\n",
    "np.save(\"val_target_small\", data.target_val[:ROWS_TO_KEEP, :])\n",
    "np.save(\"scoring_input_small\", data.input_scoring[:ROWS_TO_KEEP, :])\n",
    "np.save(\"scoring_target_small\", data.target_scoring[:ROWS_TO_KEEP, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train constant prediction model\n",
    "\n",
    "$\\hat{y} = E[y_{train}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_model = data.target_train.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multiple linear regression model\n",
    "\n",
    "$\\beta = {(X_{train}^TX_{train})}^{-1}X_{train}^Ty_{train}$\n",
    "\n",
    "$\\hat{y} = X_{input}^T \\beta$\n",
    "\n",
    "where $X_{train}$ and $X_{input}$ correspond to the training data and the input data you would like to inference on, respectively. $X_{train}$ and $X_{input}$ both have a column of ones concatenated to the feature space for the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding bias unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.input_train\n",
    "bias_vector = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((X, bias_vector), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_weights = np.linalg.inv(X.transpose()@X)@X.transpose()@data.target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# train your model here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Prediction\n",
    "const_pred_val = np.repeat(const_model[np.newaxis, :], data.target_val.shape[0], axis = 0)\n",
    "print(const_pred_val.shape)\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X_val = data.input_val\n",
    "bias_vector_val = np.ones((X_val.shape[0], 1))\n",
    "X_val = np.concatenate((X_val, bias_vector_val), axis=1)\n",
    "mlr_pred_val = X_val@mlr_weights\n",
    "print(mlr_pred_val.shape)\n",
    "\n",
    "# Load your prediction here\n",
    "\n",
    "# Load predictions into data_utils object\n",
    "data.model_names = ['const', 'mlr'] # add names of your models here\n",
    "preds = [const_pred_val, mlr_pred_val] # add your custom predictions here\n",
    "data.preds_val = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reweight_target(data_split = 'val')\n",
    "data.reweight_preds(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_val\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained models with different hyperparameters, use the ones that performed the best on validation data for evaluation on scoring data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on scoring data\n",
    "\n",
    "#### Do this at the VERY END (when you have finished tuned the hyperparameters for your  model and are seeking a final evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_input_path = data_path + 'scoring_input.npy'\n",
    "scoring_target_path = data_path + 'scoring_target.npy'\n",
    "\n",
    "# path to target input\n",
    "data.input_scoring = np.load(scoring_input_path)\n",
    "\n",
    "# path to target output\n",
    "data.target_scoring = np.load(scoring_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant prediction\n",
    "const_pred_scoring = np.repeat(const_model[np.newaxis, :], data.target_scoring.shape[0], axis = 0)\n",
    "print(const_pred_scoring.shape)\n",
    "\n",
    "# multiple linear regression\n",
    "X_scoring = data.input_scoring\n",
    "bias_vector_scoring = np.ones((X_scoring.shape[0], 1))\n",
    "X_scoring = np.concatenate((X_scoring, bias_vector_scoring), axis=1)\n",
    "mlr_pred_scoring = X_scoring@mlr_weights\n",
    "print(mlr_pred_scoring.shape)\n",
    "\n",
    "# Your model prediction here\n",
    "\n",
    "# Load predictions into object\n",
    "data.model_names = ['const', 'mlr'] # model name here\n",
    "preds = [const_pred_scoring, mlr_pred_scoring] # add prediction here\n",
    "data.preds_scoring = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight predictions and target\n",
    "data.reweight_target(data_split = 'scoring')\n",
    "data.reweight_preds(data_split = 'scoring')\n",
    "\n",
    "# set and calculate metrics\n",
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_scoring\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
